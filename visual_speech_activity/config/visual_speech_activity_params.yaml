# ROS2 Parameters for Visual Speech Activity Detection Node
# This file contains all configurable parameters for the VisualSpeechActivityNode

visual_speech_activity_node:
  ros__parameters:
    # Topic Configuration
    recognition_input_topic: "/humans/faces/recognized"
    landmarks_input_topic: "/humans/faces/detected"
    output_topic: "/humans/faces/speaking"
    output_image_topic: "/humans/faces/speaking/annotated_img"
    
    # ROS4HRI Configuration
    # Set to true for per-ID topics (/humans/faces/<id>/speaking)
    # Set to false for array mode (/humans/faces/speaking with FacialRecognitionArray)
    ros4hri_with_id: true
    
    # VSDLM Parameters
    # VSDLM (Visual Speech Detection Lightweight Model) by PINTO0309
    # Reference: https://github.com/PINTO0309/VSDLM
    # License: MIT
    
    # Probability threshold for classifying as speaking (0.0-1.0)
    # Higher values = fewer false positives but may miss speaking
    # Recommended: 0.4-0.6
    speaking_threshold: 0.9
    
    # Directory containing VSDLM weights (relative to package source)
    vsdlm_weights_path: "weights"
    
    # Specific VSDLM weights filename
    vsdlm_weights_name: "vsdlm_l.onnx"
    
    # Model variant for auto-download: P/N/S/M/L
    # P: 112KB, F1=0.9502, latency=0.18ms (ultra-fast)
    # N: 176KB, F1=0.9586, latency=0.31ms (very fast)
    # S: 494KB, F1=0.9696, latency=0.50ms (recommended - good balance)
    # M: 1.7MB, F1=0.9801, latency=0.70ms (high accuracy)
    # L: 6.4MB, F1=0.9891, latency=0.91ms (maximum accuracy)
    vsdlm_model_variant: "L"
    
    # ONNX execution provider: cpu, cuda, tensorrt
    # cpu: Works everywhere, slower
    # cuda: Requires NVIDIA GPU with CUDA, faster
    # tensorrt: Requires TensorRT, fastest on NVIDIA GPUs
    vsdlm_execution_provider: "cpu"
    
    # YOLO Landmark Mode Parameters
    # When using YOLO 5-point landmarks instead of dlib 68-point landmarks,
    # the mouth WIDTH is taken directly from the distance between the two mouth corner landmarks.
    # Only the HEIGHT needs to be estimated as a ratio of the face bbox height.
    
    # Mouth crop height as ratio of face bbox height (default: 0.35)
    # Increase if mouth region is too small vertically, decrease if too large
    # Typical range: 0.25 - 0.45
    vsdlm_mouth_height_ratio: 0.10
    
    # Debug Configuration
    # Enable verbose debug output with logger.info
    enable_debug_output: true
    
    # Save mouth crops to /tmp for debugging (every 30th frame)
    # Useful for verifying what the model sees
    vsdlm_debug_save_crops: true
    
    # Camera Image Topics (same pattern as face_recognition)
    # Use either regular image topic OR compressed topic (not both)
    
    # Regular image topic (uncompressed)
    image_topic: ""
    
    # Compressed image topic (optional, recommended for bandwidth)
    # Leave empty to use regular image_topic
    # Example: "/camera/color/image_raw/compressed"
    compressed_topic: "/camera/image_raw/compressed"
    
    # Face Recognition Integration
    # Set to true to use face recognition for robust identity tracking (recommended)
    # Set to false to work with face_id only (fallback when face recognition unavailable)
    use_face_recognition: false
    
    # Image Visualization
    # Enable/disable annotated image output with lip visualization
    enable_image_output: true
    
    # Debug Configuration
    enable_debug_output: true
